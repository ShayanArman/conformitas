import type { NextApiRequest, NextApiResponse } from "next";

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  if (req.method !== "POST") {
    return res.status(405).json({ error: "Method not allowed" });
  }

  const { prompt } = req.body;
  
  if (!prompt) {
    return res.status(400).json({ error: "Prompt is required" });
  }

  // Set up SSE headers
  res.writeHead(200, {
    "Content-Type": "text/event-stream",
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Headers": "Content-Type",
  });

  try {
    const functionName = "GoogleInvokeModelStream-workflows-development";
    const payload = {
      input: { prompt },
      modelId: 'gemini-2.5-flash'
    };

    const stream = await streamInvokeLambda({ functionName, payload });
    
    for await (const chunk of stream) {
      res.write(`data: ${JSON.stringify({ chunk, done: false })}\n\n`);
      // Force flush the response to send the chunk immediately
      if ((res as any).flush) {
        (res as any).flush();
      }
    }
    
    res.write(`data: ${JSON.stringify({ chunk: null, done: true })}\n\n`);
    res.end();
  } catch (error) {
    console.error("Error in AI streaming:", error);
    res.write(`data: ${JSON.stringify({ error: error instanceof Error ? error.message : "Unknown error" })}\n\n`);
    res.end();
  }
}